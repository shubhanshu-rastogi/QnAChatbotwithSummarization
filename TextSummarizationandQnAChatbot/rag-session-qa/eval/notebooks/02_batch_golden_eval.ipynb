{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch DeepEval (Match Summary)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# If needed, uncomment to install/upgrade DeepEval\n",
        "# !pip install -U deepeval\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv('./../.env')\n",
        "\n",
        "ROOT = Path('..').resolve().parent\n",
        "sys.path.append(str(ROOT))\n",
        "\n",
        "BASE_URL = os.getenv('BASE_URL', 'http://localhost:8000')\n",
        "FILE_PATH = Path(os.getenv('SAMPLE_FILE', '../sample_docs/Match_Summary.pdf')).resolve()\n",
        "PUBLISH = os.getenv('DEEPEVAL_PUBLISH', 'false').lower() == 'true'\n",
        "\n",
        "print('Backend:', BASE_URL)\n",
        "print('File:', FILE_PATH)\n",
        "print('Publish to Confident AI:', PUBLISH)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
        "from deepeval.evaluate import evaluate, AsyncConfig\n",
        "from deepeval.metrics import (\n",
        "    ContextualPrecisionMetric,\n",
        "    ContextualRecallMetric,\n",
        "    AnswerRelevancyMetric,\n",
        "    FaithfulnessMetric,\n",
        ")\n",
        "\n",
        "try:\n",
        "    from deepeval.metrics import ContextualRelevancyMetric\n",
        "except Exception:\n",
        "    ContextualRelevancyMetric = None\n",
        "\n",
        "try:\n",
        "    from deepeval.metrics import CompletenessMetric\n",
        "except Exception:\n",
        "    CompletenessMetric = None\n",
        "\n",
        "try:\n",
        "    from deepeval.metrics import GEval\n",
        "except Exception:\n",
        "    GEval = None\n",
        "\n",
        "def build_metrics():\n",
        "    metrics = [\n",
        "        ContextualPrecisionMetric(),\n",
        "        ContextualRecallMetric(),\n",
        "        AnswerRelevancyMetric(),\n",
        "        FaithfulnessMetric(),\n",
        "    ]\n",
        "    if ContextualRelevancyMetric is not None:\n",
        "        metrics.append(ContextualRelevancyMetric())\n",
        "    elif GEval is not None:\n",
        "        metrics.append(\n",
        "            GEval(\n",
        "                name='Context Relevance',\n",
        "                criteria='Evaluate how relevant the retrieval context is to the question. Score 0 to 1.',\n",
        "                evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT],\n",
        "            )\n",
        "        )\n",
        "    if CompletenessMetric is not None:\n",
        "        metrics.append(CompletenessMetric())\n",
        "    elif GEval is not None:\n",
        "        metrics.append(\n",
        "            GEval(\n",
        "                name='Completeness',\n",
        "                criteria='Assess if the answer is complete given the context. Score 0 to 1.',\n",
        "                evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT],\n",
        "            )\n",
        "        )\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "questions = [\n",
        "    'How many runs did Hardik Pandya score?',\n",
        "    'How many balls did Hardik Pandya face?',\n",
        "    'How many sixes did Hardik Pandya hit?',\n",
        "    'How many fours did Hardik Pandya hit?',\n",
        "    'How many runs did Tilak Varma score?',\n",
        "    'How many sixes did Tilak Varma hit?',\n",
        "    'How many fours did Tilak Varma hit?',\n",
        "    'How many runs did Rinku Singh score?',\n",
        "    'Who dismissed Suryakumar Yadav?',\n",
        "    'Who caught Hardik Pandya?',\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "with open(FILE_PATH, 'rb') as f:\n",
        "    files = {'file': (FILE_PATH.name, f)}\n",
        "    upload_res = requests.post(f'{BASE_URL}/upload', files=files)\n",
        "\n",
        "upload_res.raise_for_status()\n",
        "session_id = upload_res.json().get('session_id')\n",
        "print('Session:', session_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ask + Evaluate Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "cases = []\n",
        "rows = []\n",
        "for q in questions:\n",
        "    payload = {'session_id': session_id, 'question': q}\n",
        "    ask_res = requests.post(f'{BASE_URL}/ask', json=payload)\n",
        "    ask_res.raise_for_status()\n",
        "    ask_data = ask_res.json()\n",
        "    answer = ask_data.get('answer', '')\n",
        "    retrieval_context = ask_data.get('retrieval_context', [])\n",
        "    case = LLMTestCase(input=q, actual_output=answer, retrieval_context=retrieval_context)\n",
        "    cases.append(case)\n",
        "\n",
        "    metric_scores = {}\n",
        "    for metric in build_metrics():\n",
        "        metric.measure(case)\n",
        "        name = getattr(metric, 'name', metric.__class__.__name__)\n",
        "        metric_scores[name] = {\n",
        "            'score': getattr(metric, 'score', None),\n",
        "            'reason': getattr(metric, 'reason', None),\n",
        "        }\n",
        "\n",
        "    rows.append({\n",
        "        'question': q,\n",
        "        'answer': answer,\n",
        "        'metrics': metric_scores,\n",
        "    })\n",
        "\n",
        "evaluate(\n",
        "    test_cases=cases,\n",
        "    metrics=build_metrics(),\n",
        "    async_config=AsyncConfig(run_async=False)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Flattened Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def flatten_metrics(metrics_dict):\n",
        "    flat = {}\n",
        "    for name, payload in metrics_dict.items():\n",
        "        flat[f'{name}_score'] = payload.get('score')\n",
        "        flat[f'{name}_reason'] = payload.get('reason')\n",
        "    return flat\n",
        "\n",
        "flat_rows = []\n",
        "for row in rows:\n",
        "    flat = {\n",
        "        'question': row['question'],\n",
        "        'answer': row['answer']\n",
        "    }\n",
        "    flat.update(flatten_metrics(row['metrics']))\n",
        "    flat_rows.append(flat)\n",
        "\n",
        "pd.DataFrame(flat_rows)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}